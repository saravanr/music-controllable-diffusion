\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{tabularx}
\graphicspath{{./}}
\usepackage{amsmath}
\DeclareMathOperator\arctanh{arctanh}
\title{Multi instrument music generation using VAEs}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to sbreak the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Saravana~Rathinam\thanks{Project for course CS236 - Generative Models} \\
  \\
  Stanford University\\
  \texttt{saravanr@stanford.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle


\begin{abstract}
MIDI format gives a compressed and quantized representation of
a Music which may give us a tractable way to generate long melodies. In this project we generate MIDI audio using Variation Auto Encoders (VAE) for multiple instruments. In addition, we generate MIDI sequences given a short begining sequence and let the model fill in the rest using a Conditional-VAE. We evaluate several $\beta$-VAE's by comparing their FID scores. We also publish a MIDI encoded numpy dataset of 140, 944 samples (~2.7 Gb) that can be used for further research. Finally we discuss some tips and tricks that were useful in achieving faster training of generative models.
\end{abstract}
\section{Motivation}
\label{submission}
Composing music is a skill that is acquired by many years of practice. The music itself is the result of the life experiences of the musician, their state of mind, their unconcious and concious thoughts. Their creative talent is subjective and difficult to generalize. Recent advances in Generative models for Music generation have shown impressive results where the focus has been to replace the creative process. Learning the distribution of music creation may be an intractable problem at the moment. However one approach we can take is to build tools that serve as an aid in the creative process.  If a musician already has a few ideas in mind on how a song or melody should start, can the problem be modelled as a conditional generative process where given the start of the melody, can a model generate multiple possibilities of how the song can proceed? 

In such a generative system, the inputs to the model would be a short MIDI sequence. The system would generate a bunch of sequences that may serve as suggested next sequences and so on. By conditioning on the input and letting the musician choose the path to take, the model can help in the creative process.

\section{Related Works}

There has been a lot of good work in the area of Music creation. Google's Magenta project explores the role of machine learning in the creative process. In a recent papers \citep{mittal} built a multi-stage non autoregressive generative model that enabled using diffusion models on discrete data. They generated both unconditional music as well as conditional in-filling. They used a Denoising Diffusion Probabilistic Model \cite{ho2020denoising} on top of a MusicVAE model that generated the continuous time latent embeddings. Similarly \cite{choi2021ilvr} proposed an Iterative Latent Variable Refinement (ILVR) method to guide the DDPM to generate high quality images based on a given reference image. Also \cite{song2020denoising}  produced a way to accelerate sampling process of a DDPM which can make generation process of sequences faster. In another beautiful apprach, \cite{bazin2021} built an interactive web interface that transforms sound by inpainting. This approach is similar to what \cite{meng2021sdedit} built with SDEdit that adapts to editing tasks at test time, without the need for re-training the model.

\section{Approach}

Initially we planned to use Denoising Diffusion Probabilistic Models (DDPMs) to model the input distribution. DDPMs have been shown to generate high quality music samples \cite{mittal} and also offer controllable generation using post-hoc conditional infilling. However DDPMs operate on continuous space. In \cite{mittal} the authors used a pre-trained variational auto-encoder (VAE) to learn the continuous latent space representation of the input discrete data. This led us to first implement a VAE, whose encoder network can be used to generate continuous representation for DDPMs.

\subsection{Auto Encoder and Variational Auto Encoder}

A auto encoder consists of an encoder and decoder network. The encoder tries to convert the input $x$ to a smaller latent space representation $z$. The dimension of $z$ is typically smaller than the dimension of $x$. The decoder network of an autoencoder converts $z$ back to the input respresentation $\hat{x}$. Once trained, the decoder can be used to generate samples of $x$. The disadvantage though is that in many cases this training may not be tractable. Also the latent representation $z$ serves as a look up table instead of being a continous representation of the input, which can be interpolated along different dimensions.

A VAE \citep{kingma2014autoencoding} solves this problem by trying to learn the distribution $q(z|x)$ of the latent space and then generate $z$ by sampling from this distribution. Typically the distribution chosen is Gaussian which makes it easier to sample using the reparameterization trick. The objective for the VAE thus becomes to maximize the ELBO:

$$ ELBO(x;\theta, \phi) = \mathbb{E}_{q_{\phi}(z|x)} [ \log(p_{\theta}(x|z))] - \mathcal{D}(q_{\phi}(z|x) || p(z))$$

The first term here is the reconstruction loss and the second term the $KL$ divergence between the prior for $z$ and the one generated by the encoder $q_{\phi} (z|x)$.

\subsection{Dataset}


The dataset for the project a combination of the Lakh MIDI Dataset v0.1 \cite{colinlmd} and the MIDI dataset posted at \citep{midiman}. The Lakh MIDI data set is a collection of 176, 581 unique MIDI files out of which 45, 129 have equivalent songs in the Million Song Dataset. The \citep{midiman} collection has about 150, 000 midi files. Of these only about 140, 944 midi files were usable because of conversion errors. 

For encoding the MIDI files, We considered OctupleMIDI encoding format as proposed and implemented in \cite{zeng2021musicbert}. The encoding format was however hard to normalize and get good results. It was not clear what the mininum and maximum values were for each of the MIDI note attributes. This caused the model to generate invalid MIDI combinations which could not be converted to MIDI  files. Later we moved to using Google's Magenta \citep{magenta} note sequence library. This encodes the MIDI file into a list of note sequences.

Magenta converts a MIDI file to a sequence of notes. Each note has:

\begin{enumerate}
\item \textbf{Pitch}: The frequencey of the note
\item \textbf{Velocity}: The intensity of the note
\item \textbf{Instrument}: The instrument where the note should be played (or synthesized)
\item \textbf{Program}: A control message that specifies which instrument should be selected to play the note
\item \textbf{Start time}: The start time of the note (seconds)
\item \textbf{End time}: The end time of the note (seconds)
\end{enumerate}


\subsection{Encoding and Normalization}

In the first few implementations of the VAE we did the mistake of not normalizing the inputs. The pitch, velocity, instrument and program are discrete variables which can take values between $[0, 127]$ (although in some MIDI files, the authors found this was not true. These files were excluded from the dataset). While the start time and end time are continuous time measurements in seconds. The initial normalization approach we took was to ensure the inputs were between $[-1.0, 1.0]$. This was done by dividing the inputs by $127.0$ and then using coverting them to $[-1.0, 1.0]$by using $\tanh(x/127.0)$. The decoders output was then re-normalized by doing the inverse $\arctanh(\hat{x}) * 127.0$. This approach did not work. The decoder produced outputs which were not valid (as per MIDI spec). And the generated music sounded annoying and like noise.

The problem was with some of the inputs being categorical variables instead of continuous variables. This led us to consider alternative ways to encode the categorical variables. Initially we considered one-hot encoding, however this lead to a huge amount of memory consumption on GPU. Instead we used an embedding layer to encode these categorical variables into embedding space. The weights of the embedding layer were then used to translate back to the original categorical space. To do this we computed the $L2$ distance and then tried to find the index of lowest distance. $$ x_{categorical} = \arg \min || \mathrm{embedding.weight} - x_{decoder}||_2 $$. This approach worked much better in practice while not consuming too much memory.


\subsection{Model Architecture}

A traditional VAE has a single encoder and a single decoder. We had some inputs that are categorical variables and some that are continuous. We also want to generate conditional output, where the VAE is able to fill in the rest of the music sequence given the beginning. 

The reconstruction loss for categorical variables is best measured through categorical cross-entropy, while we use the $L2$ norm for continuous output. Thus our overall loss function becomes a combination sum of $KL$ divergence loss ($\mathbb{D}(q_{\phi}(z|x) || p(z))$), reconstruction losses (time and duration) and categorical cross-entropy loss (for pitch, velocity, instruments and program). 

$$ \underbrace{\mathbb{E}_{q_{\phi}(z|x_{time})} [ \log(p_{\theta}(x_{time}|z))] + \mathbb{E}_{q_{\phi}(z|x_{duration})} [ \log(p_{\theta}(x_{duration}|z))]}_{\text{L2 Reconstruction Loss}}  \underbrace{- \mathbb{D}(q_{\phi}(z|x) || p(z)) }_{\text{KL Divergence}} + $$ 

$$ \underbrace{ \mathbb{E}_{q_{\phi}(z|x,x_{partial})} [ \log(p_{\theta}(x|z, x_{partial}))]}_{\text{L2 Conditional Reconstruction Loss}} \underbrace{-\mathbb{D}(q_{\phi}(z|x, x_{partial}) || p(z))}_{\text{Conditional KL Divergence}} $$

$$ \underbrace{-\sum_{i=1}^{128} x_{pitch} \cdot \mathrm{log}\; {\hat{x}}_{pitch}  - \sum_{i=1}^{128} x_{velocity} \cdot \mathrm{log}\; {\hat{x}}_{velocity} - \sum_{i=1}^{128} x_{instr} \cdot \mathrm{log}\; {\hat{x}}_{inst} -\sum_{i=1}^{128} x_{program} \cdot \mathrm{log}\; {\hat{x}}_{program}}_{\text{Categorical Cross Entropy Losses}} $$

Initially we built the encoder using only Linear layers. However later we relalized that since music is inherently auto-regressive, a RNN network may be better suited. After experimenting with RNN, LSTMs and GRU layers, we finally settled with bi-directional GRU as one of the layers of the encoder. The encoder's output was finally passed through two additional linear layers to extract $\mu$ and $ \log(\sigma)$.

\begin{center}
\begin{figure}
\includegraphics[width=1.0\linewidth]{encoder.png}
\caption{Encoder Architecture}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\includegraphics[width=1.0\linewidth]{decoder.png}
\caption{Decoder Architecture}
\end{figure}
\end{center}

\begin{center}
\begin{figure}
\includegraphics[width=1.0\linewidth]{midi_vae.png}
\caption{Conditional VAE Architecture with different Decoders for Categorical and Continuous Variables}
\end{figure}
\end{center}

As illustrated in the figures above, the VAE architecture comprises of an encoder and multiple decoders which share the same $z$ space. This sharing of space while backpropagating the loss ensures that the VAE not only optimizes the reconstruction and $KL$ divergence losses, but also ensure the conditional VAE losses are also optimized.

\subsection{Evaluation}

The evaluation of the models was done by both measuring the test loss and computing the Fréchet Inception Distance (FID) as defined by \cite{heusel2018gans} and implemented in the pytorch fid package by \cite{Seitzer2020FID}, on the validation set. The rule of thumb was to ensure that test loss decreases as training proceeds. All the components of the loss (reconstruction, categorical cross entropy etc.) were also tracked as part of the evaluation.

The data set was split into 70\% training samples, 15\% test samples and 15\% validation samples. This gave us about 98660 files in training split and 21142 files each in test and validation split. The test samples were used for computing test loss and conditional generation of music. The validation split was used to measure FID score.

FID score was measured by sampling 21142 (validation set size) samples from the trained model every 10 epochs. The FID score of conditional generation was also computed for every experiment.

\section{Experiments and Results}

Initially we did many experiments with VAEs, but most of them failed because of the encoding problem discussed earlier. Once we figured out the proper way to encode the input (using embeddings), the following experiments were run:

\begin{itemize}
\item Baseline - Variational Auto Encoder with $\beta = 1$.
\item Beta VAE with $\beta \in [10, 100]$
\item Conditional VAE music infilling using best Beta value from above.
\end{itemize}

\subsection{Baseline}

The baseline was obtained with $\beta = 1$. The training was performed for $100$ epochs with a learning rate of $1e-3$. The test loss and training loss were found to decrease together. (Figure 4.) 
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\linewidth]{training_loss_beta_1.png}
\includegraphics[width=0.4\linewidth]{test_loss_beta_1.png}
\end{center}
\caption{Training and Test Loss for $\beta = 1$}
\end{figure}

The FID scores also reduced as the training progressed. The curve for FID is not smooth because it was computed every 10 epochs while the training and test loss were computed every epoch. (Figure 5.)

\begin{figure}
\begin{center}
\includegraphics[width=0.4\linewidth]{fid_beta_1.png}
\includegraphics[width=0.4\linewidth]{controlled_fid_beta_1.png}
\caption{FID for unconditional and conditional (controlled) generation of Music for $\beta = 1$}
\end{center}
\end{figure}

\subsection{Beta VAEs}

Several beta VAEs were trained and evaluated with $\beta$ in $[10, 20, 40, 80, 100]$. The following plots show the losses and FID scores obtained.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\linewidth]{combined_training_loss.png}
\includegraphics[width=0.4\linewidth]{combined_test_loss.png}
\caption{Training and Test Loss plots for different $\beta$ values}
\end{center}
\end{figure}

Although the training and test loss appear to be similar for all models, the FID scores show a different story.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\linewidth]{combined_fid.png}
\includegraphics[width=0.4\linewidth]{combined_fid_controlled.png}
\caption{FID for unconditional and conditional (controlled) generation of Music different $\beta$ values}
\end{center}
\end{figure}

The best scores were obtained with a $\beta$ value of $20$. This was hard to explain, since the test loss looked similar for these models. However after looking at the individual losses, we noticed a difference in how fast the categorical cross entropy loss for the sequence of instruments converged. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\linewidth]{combined_instruments.png}
\includegraphics[width=0.4\linewidth]{combined_test_instruments.png}
\caption{Categorical Cross Entropy Loss for Instruments with different $\beta$}
\end{center}
\end{figure}

Note the difference in rate of convergence for $\beta = 20$.

\subsection{Improving Training Speed}
We did about 756 training, test loss evaluation and sampling runs for the project. The initial few runs were extremely slow even though the training was done on a relatively high end GPU (NVidia RTX 3090 FE). After profiling the training loop, we found the following:

\begin{itemize}
\item Run time conversion of MIDI to PyTorch Tensors was causing memory leak.
We could not root cause the memory leak, but we suspect it to be in the magenta MIDI to note sequence conversion layer even when we used more than 24 worker processes. Anyway, instead of converting the files at run time, we decided to cache the numpy files separately.
\item Individual numpy files were slow to load. The data module had to load 140, 944 files and fetching them individually was slow. Since we were IO bottlenecked, one option would have been to use python worker threads. However we decided to instead cache all the files into a single numpy file thereby loading it only once. The file was also copied over to \textit{\textbackslash dev\textbackslash shm} (shared memory), so the file stayed entirely in memory.
\item Loading across multiple workers was slow. Even though the file was in memory, the CPU had to do a lot of work transfering over the data to the GPU. Although the PCI bus is fast enough, for large quantities of data that is repeatedly fetched, this turned out to be a big bottleneck. To solve this we copied over all the data to the GPU on startup. The tensors took about 4Gb of GPU memory. This transfer however had to be done using a single worker process, since multiple processes cannot access the same CUDA address space.

We also experimented with different batch sizes and found a batch size of 2048 to be good enough. By doing the above we were able to speed up the training from about 90 mins to about 10 mins. This order of magnitude increase in training and eval speed allowed us to train many variations of the model in a short time.

\end{itemize}


\section{Conclusion}

This project was a step in trying to generate long sequences of music (90 seconds+) in a more tractable way. The music generated was also multi-instrument and we also tried unconditional generation. The conditional generation was not as succesful because we conditioned on the begining of the song, which was mostly silent or had a few instruments/notes. However the unconditional generating produced some interesting and pleasant music \citep{generated_music}. We also published a dataset \citep{midi_note_seq_data_set} that can be used for further research along with the source code for encoding and decoding \citep{vae_source}.

We found the best $\beta$ value for a good FID score, with the given model architecture, to be $\beta = 20$. The cross entropy loss for the sequence and number of instruments in the generated MIDI file, for good FID models seems to converge slower. Its difficult to explain why there is this correlation, but our intution is that instruments tend to be grouped for different song genres and perhaps the model learns this better when the instruments loss converges slower.


Generative modeling is a fascinating area of research. A lot of academic effort has gone into generative models, but their validation is mostly limited to generating images. We can effortlessly classify an image as high quality or low quality by just looking at it. However other types of data, including Music need more effort. Even though FID scores can serve as a good metric to evaluate against, still music seems to be a process that may not conform to a particular distribution or reside on a particular manifold. The best quality music seem to be the one in outliers and these are really hard to find.


\bibliography{project}
\bibliographystyle{icml2021}

\end{document}


\section{Submission of papers to NeurIPS 2021}

Please read the instructions below carefully and follow them faithfully.

\subsection{Style}

Papers to be submitted to NeurIPS 2021 must be prepared according to the
instructions presented here. Papers may only be up to {\bf nine} pages long,
including figures. Additional pages \emph{containing only acknowledgments and
references} are allowed. Papers that exceed the page limit will not be
reviewed, or in any other way considered for presentation at the conference.

The margins in 2021 are the same as those in 2007, which allow for $\sim$$15\%$
more words in the paper compared to earlier years.

Authors are required to use the NeurIPS \LaTeX{} style files obtainable at the
NeurIPS website as indicated below. Please make sure you use the current files
and not previous versions. Tweaking the style files may be grounds for
rejection.

\subsection{Retrieval of style files}

The style files for NeurIPS and other conference information are available on
the World Wide Web at
\begin{center}
  \url{http://www.neurips.cc/}
\end{center}
The file \verb+neurips_2021.pdf+ contains these instructions and illustrates the
various formatting requirements your NeurIPS paper must satisfy.

The only supported style file for NeurIPS 2021 is \verb+neurips_2021.sty+,
rewritten for \LaTeXe{}.  \textbf{Previous style files for \LaTeX{} 2.09,
  Microsoft Word, and RTF are no longer supported!}

The \LaTeX{} style file contains three optional arguments: \verb+final+, which
creates a camera-ready copy, \verb+preprint+, which creates a preprint for
submission to, e.g., arXiv, and \verb+nonatbib+, which will not load the
\verb+natbib+ package for you in case of package clash.

\paragraph{Preprint option}
If you wish to post a preprint of your work online, e.g., on arXiv, using the
NeurIPS style, please use the \verb+preprint+ option. This will create a
nonanonymized version of your work with the text ``Preprint. Work in progress.''
in the footer. This version may be distributed as you see fit. Please \textbf{do
  not} use the \verb+final+ option, which should \textbf{only} be used for
papers accepted to NeurIPS.

At submission time, please omit the \verb+final+ and \verb+preprint+
options. This will anonymize your submission and add line numbers to aid
review. Please do \emph{not} refer to these line numbers in your paper as they
will be removed during generation of camera-ready copies.

The file \verb+neurips_2021.tex+ may be used as a ``shell'' for writing your
paper. All you have to do is replace the author, title, abstract, and text of
the paper with your own.

The formatting instructions contained in these style files are summarized in
Sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).  Use 10~point
type with a vertical spacing (leading) of 11~points.  Times New Roman is the
preferred typeface throughout, and will be selected for you by default.
Paragraphs are separated by \nicefrac{1}{2}~line space (5.5 points), with no
indentation.

The paper title should be 17~point, initial caps/lower case, bold, centered
between two horizontal rules. The top rule should be 4~points thick and the
bottom rule should be 1~point thick. Allow \nicefrac{1}{4}~inch space above and
below the title to rules. All pages should start at 1~inch (6~picas) from the
top of the page.

For the final version, authors' names are set in boldface, and each name is
centered above the corresponding address. The lead author's name is to be listed
first (left-most), and the co-authors' names (if different address) are set to
follow. If there is only one co-author, list both author and co-author side by
side.

Please pay special attention to the instructions in Section \ref{others}
regarding figures, tables, acknowledgments, and references.

\section{Headings: first level}
\label{headings}

All headings should be lower case (except for first word and proper nouns),
flush left, and bold.

First-level headings should be in 12-point type.

\subsection{Headings: second level}

Second-level headings should be in 10-point type.

\subsubsection{Headings: third level}

Third-level headings should be in 10-point type.

\paragraph{Paragraphs}

There is also a \verb+\paragraph+ command available, which sets the heading in
bold, flush left, and inline with the text, with the heading followed by 1\,em
of space.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone.

\subsection{Citations within the text}

The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2021+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2021}
\end{verbatim}

As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.

You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table number and
title always appear before the table.  See Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical rules.} We
strongly suggest the use of the \verb+booktabs+ package, which allows for
typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts are also
  acceptable for NeurIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.

\subsection{Margins in \LaTeX{}}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.

\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2021/PaperInformation/FundingDisclosure}.

Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references.
Note that the Reference section does not count towards the page limit.
\medskip

{
\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
%%% END INSTRUCTIONS %%%

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerTODO{}
  \item Did you describe the limitations of your work?
    \answerTODO{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerTODO{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerTODO{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerTODO{}
	\item Did you include complete proofs of all theoretical results?
    \answerTODO{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerTODO{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerTODO{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerTODO{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerTODO{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerTODO{}
  \item Did you mention the license of the assets?
    \answerTODO{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerTODO{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerTODO{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerTODO{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerTODO{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerTODO{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerTODO{}
\end{enumerate}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Appendix}

Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
This section will often be part of the supplemental material.

\end{document}



% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
